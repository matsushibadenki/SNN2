# matsushibadenki/snn/train.py
# (æ—§ snn_research/training/main.py)
#
# æ–°ã—ã„çµ±åˆå­¦ç¿’å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
#
# æ©Ÿèƒ½:
# - ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ— ãƒ•ã‚§ãƒ¼ã‚º2ã€Œ2.2. çµ±åˆã•ã‚ŒãŸå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ã«å¯¾å¿œã€‚
# - DIã‚³ãƒ³ãƒ†ãƒŠã‚’ä½¿ç”¨ã—ã¦ã€å­¦ç¿’ã«å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆãƒ¢ãƒ‡ãƒ«, ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ,
#   Optimizer, Loss, Trainerãªã©ï¼‰ã‚’å‹•çš„ã«çµ„ã¿ç«‹ã¦ã‚‹ã€‚
# - --data_format å¼•æ•°ã§ã€ç•°ãªã‚‹å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«åˆ‡ã‚Šæ›¿ãˆå¯èƒ½ã€‚
# - --override_config å¼•æ•°ã§ã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰ä»»æ„ã®è¨­å®šã‚’ä¸Šæ›¸ãã§ãã‚‹ã€‚
# - åˆ†æ•£å­¦ç¿’ (`--distributed`) ã«ã‚‚å¯¾å¿œã€‚

import argparse
import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, random_split, DistributedSampler
from dependency_injector.wiring import inject, Provide

from app.containers import TrainingContainer
from snn_research.data.datasets import get_dataset_class, DistillationDataset, DataFormat
from snn_research.training.trainers import BreakthroughTrainer, DistillationTrainer

# DIã‚³ãƒ³ãƒ†ãƒŠã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
container = TrainingContainer()
# è¨­å®šã®ãƒãƒ¼ã‚¸ï¼ˆãƒ•ã‚¡ã‚¤ãƒ« -> ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ï¼‰
# ... (mainé–¢æ•°å†…ã§å®Ÿè¡Œ)

@inject
def train(
    args,
    snn_model: BreakthroughSNN = Provide[TrainingContainer.snn_model],
    tokenizer = Provide[TrainingContainer.tokenizer],
    config = Provide[TrainingContainer.config]
):
    """å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    is_distributed = args.distributed
    rank = int(os.environ.get("LOCAL_RANK", -1))
    
    device = f'cuda:{rank}' if is_distributed else get_auto_device()
    snn_model.to(device)

    # --- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™ ---
    is_distillation = config.training.type() == "distillation"
    if is_distillation:
        dataset = DistillationDataset(
            file_path=os.path.join(args.data_path, "distillation_data.jsonl"),
            data_dir=args.data_path,
            tokenizer=tokenizer,
            max_seq_len=snn_model.time_steps
        )
    else:
        DatasetClass = get_dataset_class(DataFormat(config.data.format()))
        dataset = DatasetClass(
            file_path=args.data_path,
            tokenizer=tokenizer,
            max_seq_len=snn_model.time_steps
        )
        
    train_size = int((1.0 - config.data.split_ratio()) * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    train_sampler = DistributedSampler(train_dataset) if is_distributed else None
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.training.batch_size(),
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        collate_fn=collate_fn(tokenizer)
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.training.batch_size(),
        shuffle=False,
        collate_fn=collate_fn(tokenizer)
    )

    # --- DIã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰å­¦ç¿’ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å–å¾— ---
    optimizer = container.optimizer(params=snn_model.parameters())
    scheduler = container.scheduler(optimizer=optimizer) if config.training.use_scheduler() else None

    if is_distributed:
        snn_model = DDP(snn_model, device_ids=[rank])
    
    # ã‚¢ã‚¹ãƒˆãƒ­ã‚µã‚¤ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’åˆæœŸåŒ– (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
    astrocyte = container.astrocyte_network(snn_model=snn_model) if args.use_astrocyte else None

    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’é¸æŠã—ã¦åˆæœŸåŒ–
    if is_distillation:
        trainer: DistillationTrainer = container.distillation_trainer(
            model=snn_model, optimizer=optimizer, scheduler=scheduler, device=device, rank=rank, astrocyte_network=astrocyte
        )
    else:
        trainer: BreakthroughTrainer = container.standard_trainer(
            model=snn_model, optimizer=optimizer, scheduler=scheduler, device=device, rank=rank, astrocyte_network=astrocyte
        )

    # --- å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œ ---
    print(f"ğŸš€ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ (Device: {device}, Distributed: {is_distributed})")
    start_epoch = 0
    if args.resume_path:
        start_epoch = trainer.load_checkpoint(args.resume_path)

    for epoch in range(start_epoch, config.training.epochs()):
        if is_distributed:
            train_sampler.set_epoch(epoch)
        
        train_metrics = trainer.train_epoch(train_loader, epoch)
        
        if rank in [-1, 0] and (epoch % config.training.eval_interval() == 0 or epoch == config.training.epochs() - 1):
            val_metrics = trainer.evaluate(val_loader, epoch)
            
            if epoch % config.training.log_interval() == 0:
                checkpoint_path = os.path.join(config.training.log_dir(), f"checkpoint_epoch_{epoch}.pth")
                trainer.save_checkpoint(
                    path=checkpoint_path,
                    epoch=epoch,
                    metric_value=val_metrics.get('total', float('inf')),
                    tokenizer_name=config.data.tokenizer_name(),
                    config=config.model.to_dict()
                )

    if rank in [-1, 0]:
        print("âœ… å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

def collate_fn(tokenizer):
    def collate(batch):
        # ãƒãƒƒãƒå†…ã®æœ€å¤§ã®é•·ã•ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        inputs = [item[0] for item in batch]
        targets = [item[1] for item in batch]
        
        # Distillationã®å ´åˆã€teacher_logitsã‚‚ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        if len(batch[0]) > 2:
            logits = [item[2] for item in batch]
            padded_logits = torch.nn.utils.rnn.pad_sequence(logits, batch_first=True, padding_value=0)
            
            padded_inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)
            padded_targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=tokenizer.pad_token_id)

            return padded_inputs, padded_targets, padded_logits
        else:
            padded_inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)
            padded_targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=tokenizer.pad_token_id)
            return padded_inputs, padded_targets
    return collate


def get_auto_device() -> str:
    if torch.cuda.is_available(): return "cuda"
    if torch.backends.mps.is_available(): return "mps"
    return "cpu"

def main():
    parser = argparse.ArgumentParser(description="SNN çµ±åˆå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument("--config", type=str, default="configs/base_config.yaml", help="åŸºæœ¬è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--model_config", type=str, required=True, help="ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--data_path", type=str, help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹")
    parser.add_argument("--override_config", type=str, action='append', help="è¨­å®šã‚’ä¸Šæ›¸ã (ä¾‹: 'training.epochs=5')")
    parser.add_argument("--distributed", action="store_true", help="åˆ†æ•£å­¦ç¿’ã‚’æœ‰åŠ¹ã«ã™ã‚‹")
    parser.add_argument("--resume_path", type=str, help="ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å­¦ç¿’ã‚’å†é–‹ã™ã‚‹")
    parser.add_argument("--use_astrocyte", action="store_true", help="ã‚¢ã‚¹ãƒˆãƒ­ã‚µã‚¤ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æœ‰åŠ¹ã«ã™ã‚‹")
    
    args = parser.parse_args()

    # DIã‚³ãƒ³ãƒ†ãƒŠã®è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰
    container.config.from_yaml(args.config)
    container.config.from_yaml(args.model_config)
    if args.data_path:
        container.config.data.path.from_value(args.data_path)
    if args.override_config:
        for override in args.override_config:
            key, value = override.split('=', 1)
            container.config.from_dict({key: value})

    # DDPã®åˆæœŸåŒ–
    if args.distributed:
        dist.init_process_group(backend="nccl")

    train(args)

    if args.distributed:
        dist.destroy_process_group()


if __name__ == "__main__":
    main()
