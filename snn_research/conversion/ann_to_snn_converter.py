# matsushibadenki/snn2/snn_research/conversion/ann_to_snn_converter.py
# GGUF/Safetensorså½¢å¼ã®ANNãƒ¢ãƒ‡ãƒ«ã‹ã‚‰SNNã¸ã®å¤‰æ›ãƒ»è’¸ç•™ã‚’è¡Œã†ã‚³ãƒ³ãƒãƒ¼ã‚¿
#
# æ©Ÿèƒ½:
# - æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰Safetensorsã¾ãŸã¯GGUFãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
# - ANN-SNNå¤‰æ›: ANNã®é‡ã¿ã‚’SNNãƒ¢ãƒ‡ãƒ«ã«ç›´æ¥ã‚³ãƒ”ãƒ¼ã™ã‚‹ã€‚
# - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çŸ¥è­˜è’¸ç•™: ANNã‚’æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€SNNã‚’å­¦ç¿’ã•ã›ã‚‹ã€‚
# - é–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã€å¤‰æ›å¾Œã®SNNã®æ´»å‹•ã‚’å®‰å®šã•ã›ã‚‹ã€‚

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from safetensors.torch import load_file
from tqdm import tqdm  # type: ignore
from typing import Dict, Any, Optional, Iterator

from snn_research.benchmark.ann_baseline import ANNBaselineModel
from snn_research.core.snn_core import AdaptiveLIFNeuron, BreakthroughSNN

# GGUFãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ (å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ggufãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨)
def _load_gguf_placeholder(path: str) -> Dict[str, torch.Tensor]:
    print(f"âš ï¸ GGUFãƒ­ãƒ¼ãƒ€ãƒ¼ã¯ç¾åœ¨ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã§ã™ã€‚'{path}' ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ã¾ã™ã€‚")
    # å®Ÿéš›ã®ggufãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯å¤–éƒ¨ä¾å­˜ãªã®ã§ã€ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼ã®state_dictã‚’è¿”ã—ã¾ã™ã€‚
    # å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ `gguf` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒ­ãƒ¼ãƒ€ãƒ¼ã«ç½®ãæ›ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    return {
        "token_embedding.weight": torch.randn(32000, 128),
        "output_projection.weight": torch.randn(32000, 128),
        # ... ãã®ä»–ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é‡ã¿
    }

class AnnToSnnConverter:
    """
    æ—¢å­˜ã®ANNãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰SNNãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã€‚
    """
    def __init__(self, snn_model: nn.Module, model_config: Dict[str, Any]):
        """
        ã‚³ãƒ³ãƒãƒ¼ã‚¿ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

        Args:
            snn_model: å¤‰æ›å…ˆã¨ãªã‚‹SNNãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€‚
            model_config: SNNãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­å®šã€‚
        """
        self.snn_model = snn_model
        self.model_config = model_config
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        self.snn_model.to(self.device)

    def _load_ann_weights(self, ann_model_path: str) -> Dict[str, torch.Tensor]:
        """ANNãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚"""
        print(f"ğŸ’¾ ANNãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {ann_model_path}")
        if ann_model_path.endswith(".safetensors"):
            return load_file(ann_model_path, device=self.device)
        elif ann_model_path.endswith(".gguf"):
            # GGUFã®èª­ã¿è¾¼ã¿ã¯å°‚ç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚
            # ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼ã®ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚
            return _load_gguf_placeholder(ann_model_path)
        else:
            raise ValueError("ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚.safetensorsã¾ãŸã¯.ggufã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")

    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    def calibrate_thresholds(self, calibration_loader: Any, target_rate: float = 0.1, epochs: int = 1):
        """
        å¤‰æ›å¾Œã®SNNãƒ¢ãƒ‡ãƒ«ã®ç™ºç«é–¾å€¤ã‚’ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ã€‚
        """
        print(f"âš™ï¸ ç™ºç«é–¾å€¤ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¾ã™ (ç›®æ¨™ç™ºç«ç‡: {target_rate:.2f})...")
        self.snn_model.train() # trainãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã€é©å¿œçš„é–¾å€¤ã‚’æ›´æ–°ã•ã›ã‚‹

        # é©å¿œçš„é–¾å€¤ã‚’æŒã¤ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¤ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
        lif_layers = [m for m in self.snn_model.modules() if isinstance(m, AdaptiveLIFNeuron)]
        if not lif_layers:
            print("âš ï¸ é©å¿œçš„é–¾å€¤ã‚’æŒã¤LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return

        # ç›®æ¨™ç™ºç«ç‡ã‚’è¨­å®š
        for layer in lif_layers:
            layer.target_spike_rate = target_rate

        with torch.no_grad():
            for epoch in range(epochs):
                for batch in tqdm(calibration_loader, desc=f"Calibration Epoch {epoch+1}"):
                    if isinstance(batch, (list, tuple)):
                        inputs = batch[0].to(self.device)
                    else:
                        if isinstance(batch, (list, tuple)):
                            inputs = batch[0].to(self.device)
                        else:
                            inputs = batch.to(self.device)

                    # ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ã¦AdaptiveLIFNeuronå†…ã®é–¾å€¤æ›´æ–°ãƒ­ã‚¸ãƒƒã‚¯ã‚’ãƒˆãƒªã‚¬ãƒ¼
                    self.snn_model(inputs)

        print("âœ… ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        for i, layer in enumerate(lif_layers):
            avg_threshold = layer.adaptive_threshold.mean().item()
            print(f"  - Layer {i+1} ã®å¹³å‡é–¾å€¤: {avg_threshold:.4f}")
        self.snn_model.eval() # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«æˆ»ã™

    def convert_weights(
        self,
        ann_model_path: str,
        output_path: str,
        calibration_loader: Optional[Any] = None
    ) -> None:
        """
        ANN-SNNå¤‰æ›ï¼ˆé‡ã¿ã‚³ãƒ”ãƒ¼ï¼‰ã‚’å®Ÿè¡Œã—ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§é–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†ã€‚
        """
        ann_weights = self._load_ann_weights(ann_model_path)
        snn_state_dict = self.snn_model.state_dict()

        print("ğŸ”„ ANNã®é‡ã¿ã‚’SNNãƒ¢ãƒ‡ãƒ«ã«ã‚³ãƒ”ãƒ¼ã—ã¦ã„ã¾ã™...")
        
        # ANNã¨SNNã§ã‚­ãƒ¼åãŒå¯¾å¿œã—ã¦ã„ã‚‹ã¨ä»®å®šã—ã¦ã‚³ãƒ”ãƒ¼
        # å®Ÿéš›ã«ã¯ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã«åˆã‚ã›ã¦ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦
        copied_keys = 0
        for name, param in snn_state_dict.items():
            if name in ann_weights and param.shape == ann_weights[name].shape:
                snn_state_dict[name].copy_(ann_weights[name])
                copied_keys += 1
            else:
                # å½¢çŠ¶ã‚„ã‚­ãƒ¼ãŒä¸€è‡´ã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                pass
        
        print(f"  - {copied_keys}å€‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸã€‚")
        self.snn_model.load_state_dict(snn_state_dict, strict=False)

        # é–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
        if calibration_loader:
            self.calibrate_thresholds(calibration_loader)
        
        # å¤‰æ›å¾Œã®SNNãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        torch.save({
            'model_state_dict': self.snn_model.state_dict(),
            'config': self.model_config
        }, output_path)
        print(f"âœ… é‡ã¿å¤‰æ›ãŒå®Œäº†ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ '{output_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

    def run_online_distillation(
        self,
        ann_teacher_model: nn.Module,
        dummy_data_loader: Any, # æœ¬æ¥ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        output_path: str,
        epochs: int = 3
    ) -> None:
        """
        ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çŸ¥è­˜è’¸ç•™ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
        """
        ann_teacher_model.to(self.device)
        ann_teacher_model.eval()

        optimizer = optim.AdamW(self.snn_model.parameters(), lr=1e-4)
        loss_fn = nn.KLDivLoss(reduction='batchmean', log_target=True)

        print("ğŸ”¥ ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çŸ¥è­˜è’¸ç•™ã‚’é–‹å§‹ã—ã¾ã™...")
        self.snn_model.train()

        for epoch in range(epochs):
            progress_bar = tqdm(dummy_data_loader, desc=f"Distillation Epoch {epoch+1}")
            for batch in progress_bar:
                # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
                # DataLoaderã‹ã‚‰ã®å‡ºåŠ›ãŒã‚¿ãƒ—ãƒ«ã‹ãƒã‚§ãƒƒã‚¯
                if isinstance(batch, (list, tuple)):
                    inputs = batch[0].to(self.device)
                else:
                    inputs = batch.to(self.device)
                # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
                
                optimizer.zero_grad()
                
                # SNN (ç”Ÿå¾’) ã®å‡ºåŠ›ã‚’å–å¾—
                snn_logits, _ = self.snn_model(inputs)
                
                # ANN (æ•™å¸«) ã®å‡ºåŠ›ã‚’å–å¾—
                with torch.no_grad():
                    if isinstance(ann_teacher_model, ANNBaselineModel):
                         teacher_logits = ann_teacher_model(inputs)
                    else:
                         teacher_logits = ann_teacher_model(inputs).logits

                # æå¤±ã‚’è¨ˆç®— (KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹)
                loss = loss_fn(
                    F.log_softmax(snn_logits / 2.0, dim=-1),
                    F.log_softmax(teacher_logits / 2.0, dim=-1)
                )
                
                loss.backward()
                optimizer.step()
                progress_bar.set_postfix({"loss": loss.item()})
        
        # å­¦ç¿’å¾Œã®SNNãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        torch.save({
            'model_state_dict': self.snn_model.state_dict(),
            'config': self.model_config
        }, output_path)
        print(f"âœ… çŸ¥è­˜è’¸ç•™ãŒå®Œäº†ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ '{output_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
