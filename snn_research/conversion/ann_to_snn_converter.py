# matsushibadenki/snn2/snn_research/conversion/ann_to_snn_converter.py
# GGUF/Safetensorså½¢å¼ã®ANNãƒ¢ãƒ‡ãƒ«ã‹ã‚‰SNNã¸ã®å¤‰æ›ãƒ»è’¸ç•™ã‚’è¡Œã†ã‚³ãƒ³ãƒãƒ¼ã‚¿
#
# æ©Ÿèƒ½:
# - æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰Safetensorsã¾ãŸã¯GGUFãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
# - ANN-SNNå¤‰æ›: ANNã®é‡ã¿ã‚’SNNãƒ¢ãƒ‡ãƒ«ã«ç›´æ¥ã‚³ãƒ”ãƒ¼ã™ã‚‹ã€‚
# - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çŸ¥è­˜è’¸ç•™: ANNã‚’æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€SNNã‚’å­¦ç¿’ã•ã›ã‚‹ã€‚

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F  # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸ Fã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
from safetensors.torch import load_file
from tqdm import tqdm  # type: ignore # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸ mypyã®ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦– â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
from typing import Dict, Any, Optional

# GGUFãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ (å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ggufãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨)
def _load_gguf_placeholder(path: str) -> Dict[str, torch.Tensor]:
    print(f"âš ï¸ GGUFãƒ­ãƒ¼ãƒ€ãƒ¼ã¯ç¾åœ¨ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã§ã™ã€‚'{path}' ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ã¾ã™ã€‚")
    # å®Ÿéš›ã®ggufãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯å¤–éƒ¨ä¾å­˜ãªã®ã§ã€ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼ã®state_dictã‚’è¿”ã—ã¾ã™ã€‚
    # å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ `gguf` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒ­ãƒ¼ãƒ€ãƒ¼ã«ç½®ãæ›ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    return {
        "token_embedding.weight": torch.randn(32000, 128),
        "output_projection.weight": torch.randn(32000, 128),
        # ... ãã®ä»–ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é‡ã¿
    }

class AnnToSnnConverter:
    """
    æ—¢å­˜ã®ANNãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰SNNãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã€‚
    """
    def __init__(self, snn_model: nn.Module, model_config: Dict[str, Any]):
        """
        ã‚³ãƒ³ãƒãƒ¼ã‚¿ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

        Args:
            snn_model: å¤‰æ›å…ˆã¨ãªã‚‹SNNãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€‚
            model_config: SNNãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­å®šã€‚
        """
        self.snn_model = snn_model
        self.model_config = model_config
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        self.snn_model.to(self.device)

    def _load_ann_weights(self, ann_model_path: str) -> Dict[str, torch.Tensor]:
        """ANNãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚"""
        print(f"ğŸ’¾ ANNãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {ann_model_path}")
        if ann_model_path.endswith(".safetensors"):
            return load_file(ann_model_path, device=self.device)
        elif ann_model_path.endswith(".gguf"):
            # GGUFã®èª­ã¿è¾¼ã¿ã¯å°‚ç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚
            # ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼ã®ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚
            return _load_gguf_placeholder(ann_model_path)
        else:
            raise ValueError("ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚.safetensorsã¾ãŸã¯.ggufã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")

    def convert_weights(self, ann_model_path: str, output_path: str):
        """
        ANN-SNNå¤‰æ›ï¼ˆé‡ã¿ã‚³ãƒ”ãƒ¼ï¼‰ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
        """
        ann_weights = self._load_ann_weights(ann_model_path)
        snn_state_dict = self.snn_model.state_dict()

        print("ğŸ”„ ANNã®é‡ã¿ã‚’SNNãƒ¢ãƒ‡ãƒ«ã«ã‚³ãƒ”ãƒ¼ã—ã¦ã„ã¾ã™...")
        
        # ANNã¨SNNã§ã‚­ãƒ¼åãŒå¯¾å¿œã—ã¦ã„ã‚‹ã¨ä»®å®šã—ã¦ã‚³ãƒ”ãƒ¼
        # å®Ÿéš›ã«ã¯ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã«åˆã‚ã›ã¦ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦
        for name, param in snn_state_dict.items():
            if name in ann_weights and param.shape == ann_weights[name].shape:
                snn_state_dict[name].copy_(ann_weights[name])
                print(f"  - ã‚³ãƒ”ãƒ¼æˆåŠŸ: {name}")
            else:
                print(f"  - âš ï¸ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚­ãƒ¼ä¸ä¸€è‡´ã¾ãŸã¯å½¢çŠ¶ä¸ä¸€è‡´ï¼‰: {name}")

        self.snn_model.load_state_dict(snn_state_dict)
        
        # å¤‰æ›å¾Œã®SNNãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        torch.save({
            'model_state_dict': self.snn_model.state_dict(),
            'config': self.model_config
        }, output_path)
        print(f"âœ… é‡ã¿å¤‰æ›ãŒå®Œäº†ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ '{output_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

    def run_online_distillation(
        self,
        ann_teacher_model: nn.Module,
        dummy_data_loader: Any, # æœ¬æ¥ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        output_path: str,
        epochs: int = 3
    ):
        """
        ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çŸ¥è­˜è’¸ç•™ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
        """
        ann_teacher_model.to(self.device)
        ann_teacher_model.eval()

        optimizer = optim.AdamW(self.snn_model.parameters(), lr=1e-4)
        loss_fn = nn.KLDivLoss(reduction='batchmean', log_target=True)

        print("ğŸ”¥ ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çŸ¥è­˜è’¸ç•™ã‚’é–‹å§‹ã—ã¾ã™...")
        self.snn_model.train()

        for epoch in range(epochs):
            progress_bar = tqdm(dummy_data_loader, desc=f"Distillation Epoch {epoch+1}")
            for batch in progress_bar:
                inputs = batch.to(self.device)
                
                optimizer.zero_grad()
                
                # SNN (ç”Ÿå¾’) ã®å‡ºåŠ›ã‚’å–å¾—
                snn_logits, _ = self.snn_model(inputs)
                
                # ANN (æ•™å¸«) ã®å‡ºåŠ›ã‚’å–å¾—
                with torch.no_grad():
                    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸ æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’ä¿®æ­£ â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
                    if isinstance(ann_teacher_model, ANNBaselineModel):
                         teacher_logits = ann_teacher_model(inputs)
                    else:
                         teacher_logits = ann_teacher_model(inputs).logits

                # æå¤±ã‚’è¨ˆç®— (KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹)
                loss = loss_fn(
                    F.log_softmax(snn_logits / 2.0, dim=-1),
                    F.log_softmax(teacher_logits / 2.0, dim=-1)
                )
                
                loss.backward()
                optimizer.step()
                progress_bar.set_postfix({"loss": loss.item()})
        
        # å­¦ç¿’å¾Œã®SNNãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        torch.save({
            'model_state_dict': self.snn_model.state_dict(),
            'config': self.model_config
        }, output_path)
        print(f"âœ… çŸ¥è­˜è’¸ç•™ãŒå®Œäº†ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ '{output_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
