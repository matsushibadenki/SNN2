# matsushibadenki/snn2/snn_research/core/snn_core.py
# SNNãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ã€æ¬¡ä¸–ä»£ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãªã©ã€ä¸­æ ¸ã¨ãªã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’é›†ç´„ã—ãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒª
#
# å¤‰æ›´ç‚¹:
# - mypyã‚¨ãƒ©ãƒ¼è§£æ¶ˆã®ãŸã‚ã€neuron_classã«æ˜ç¤ºçš„ãªå‹ãƒ’ãƒ³ãƒˆã‚’è¿½åŠ ã€‚

import torch
import torch.nn as nn
import torch.nn.functional as F
from spikingjelly.activation_based import surrogate, functional  # type: ignore
from typing import Tuple, Dict, Any, Optional, List, Type
import math

# --- ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¢ãƒ‡ãƒ« ---
class AdaptiveLIFNeuron(nn.Module):
    """
    é©å¿œçš„ç™ºç«é–¾å€¤ã‚’æŒã¤LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ (è¡¨ç¾åŠ›å‘ä¸Šã®ãŸã‚ã®æ¨™æº–ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³)
    """
# â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    def __init__(
        self,
        features: int,
        tau: float = 2.0,
        base_threshold: float = 1.0,
        adaptation_strength: float = 0.1,
        target_spike_rate: float = 0.02,
    ):
        super().__init__()
        self.tau = tau
        self.base_threshold = base_threshold
        self.adaptation_strength = adaptation_strength
        self.target_spike_rate = target_spike_rate
        self.surrogate_function = surrogate.ATan()
        self.mem_decay = math.exp(-1.0 / tau)
        self.register_buffer(
            "adaptive_threshold", torch.full((features,), base_threshold)
        )
        self.adaptive_threshold: torch.Tensor
        self.register_buffer("mem", torch.zeros(1, features))
        self.mem: torch.Tensor

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        if self.mem.shape[0] != x.shape[0] or self.mem.device != x.device:
            self.mem = torch.zeros(x.shape[0], x.shape[-1], device=x.device)

        self.mem = self.mem * self.mem_decay + x
        spike = self.surrogate_function(self.mem - self.adaptive_threshold)
        self.mem = self.mem * (1.0 - spike.detach())

        if self.training:
            with torch.no_grad():
                spike_rate_error = spike.mean() - self.target_spike_rate
                self.adaptive_threshold += self.adaptation_strength * spike_rate_error
                self.adaptive_threshold.clamp_(min=0.5)

        return spike, self.mem

class DendriticNeuron(nn.Module):
    """
    Phase 4: æ¨¹çŠ¶çªèµ·æ¼”ç®—ã‚’æ¨¡å€£ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã€‚
    è¤‡æ•°ã®åˆ†å²ï¼ˆdendritic branchesï¼‰ã‚’æŒã¡ã€ãã‚Œãã‚ŒãŒç•°ãªã‚‹æ™‚ç©ºé–“ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹ã€‚
    """
    def __init__(self, input_features: int, num_branches: int, branch_features: int):
        super().__init__()
        self.num_branches = num_branches
        # å„åˆ†å²ã¯ç‹¬ç«‹ã—ãŸç·šå½¢å¤‰æ›ã¨LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’æŒã¤
        self.branches = nn.ModuleList([
            nn.Sequential(
                nn.Linear(input_features, branch_features),
                AdaptiveLIFNeuron(branch_features)
            ) for _ in range(num_branches)
        ])
        # Soma (ç´°èƒä½“): å„åˆ†å²ã‹ã‚‰ã®å‡ºåŠ›ã‚’çµ±åˆã™ã‚‹
        self.soma_lif = AdaptiveLIFNeuron(branch_features * num_branches)
        self.output_projection = nn.Linear(branch_features * num_branches, input_features)

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        branch_outputs = []
        for branch in self.branches:
            # å„ãƒ–ãƒ©ãƒ³ãƒã¯åŒã˜å…¥åŠ›ã‚’å—ã‘å–ã‚‹
            branch_spike, _ = branch(x)
            branch_outputs.append(branch_spike)
        
        # å…¨ã¦ã®åˆ†å²ã‹ã‚‰ã®å‡ºåŠ›ã‚’çµåˆ
        concatenated_spikes = torch.cat(branch_outputs, dim=-1)
        
        # ç´°èƒä½“ã§çµ±åˆã—ã€æœ€çµ‚çš„ãªå‡ºåŠ›ã‚’ç”Ÿæˆ
        soma_spike, soma_mem = self.soma_lif(concatenated_spikes)
        
        # å‡ºåŠ›ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’å…ƒã®æ¬¡å…ƒã«å°„å½±
        output = self.output_projection(soma_spike)
        
        return output, soma_mem


class SNNLayerNorm(nn.Module):
    """SNNã®ãŸã‚ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«è¡Œã†LayerNorm"""
    def __init__(self, normalized_shape):
        super().__init__()
        self.norm = nn.LayerNorm(normalized_shape)
    def forward(self, x):
        return self.norm(x)

# --- äºˆæ¸¬ç¬¦å·åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ ---
class PredictiveCodingLayer(nn.Module):
    """
    äºˆæ¸¬ç¬¦å·åŒ–ã‚’å®Ÿè¡Œã™ã‚‹å˜ä¸€ã®éšå±¤ãƒ¬ã‚¤ãƒ¤ãƒ¼ã€‚
    ãƒˆãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã®äºˆæ¸¬ã¨ãƒœãƒˆãƒ ã‚¢ãƒƒãƒ—ã®è¦³æ¸¬ã‹ã‚‰èª¤å·®ã‚’è¨ˆç®—ã™ã‚‹ã€‚
    """
    def __init__(self, d_model: int, d_state: int, n_head: int, neuron_class: Type[nn.Module], neuron_params: Dict[str, Any]):
        super().__init__()
        # ç”Ÿæˆãƒ¢ãƒ‡ãƒ« (ãƒˆãƒƒãƒ—ãƒ€ã‚¦ãƒ³äºˆæ¸¬ã‚’ç”Ÿæˆ)
        self.generative_fc = nn.Linear(d_state, d_model)
        if neuron_class == DendriticNeuron:
            self.generative_neuron = neuron_class(input_features=d_model, **neuron_params)
        else:  # AdaptiveLIFNeuronã‚’æƒ³å®š
            self.generative_neuron = neuron_class(features=d_model)

        # æ¨è«–ãƒ¢ãƒ‡ãƒ« (ãƒœãƒˆãƒ ã‚¢ãƒƒãƒ—è¦³æ¸¬ã‹ã‚‰çŠ¶æ…‹ã‚’æ›´æ–°)
        self.inference_fc = nn.Linear(d_model, d_state)
        if neuron_class == DendriticNeuron:
            self.inference_neuron = neuron_class(input_features=d_state, **neuron_params)
        else:  # AdaptiveLIFNeuronã‚’æƒ³å®š
            self.inference_neuron = neuron_class(features=d_state)

        self.norm_state = SNNLayerNorm(d_state)
        self.norm_error = SNNLayerNorm(d_model)


    def forward(self, bottom_up_input: torch.Tensor, top_down_state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        # 1. ãƒˆãƒƒãƒ—ãƒ€ã‚¦ãƒ³äºˆæ¸¬ã‚’ç”Ÿæˆ
        prediction, _ = self.generative_neuron(self.generative_fc(top_down_state))

        # 2. äºˆæ¸¬èª¤å·®ã‚’è¨ˆç®— (è¦³æ¸¬ - äºˆæ¸¬)
        prediction_error = F.relu(bottom_up_input - prediction)
        prediction_error = self.norm_error(prediction_error)

        # 3. äºˆæ¸¬èª¤å·®ã«åŸºã¥ã„ã¦çŠ¶æ…‹ã‚’æ›´æ–°
        state_update, inference_mem = self.inference_neuron(self.inference_fc(prediction_error))
        
        # 4. æ®‹å·®æ¥ç¶šã¨æ­£è¦åŒ–ã§çŠ¶æ…‹ã‚’æ›´æ–°
        updated_state = self.norm_state(top_down_state + state_update)

        return updated_state, prediction_error, prediction, inference_mem

# --- ã‚³ã‚¢SNNãƒ¢ãƒ‡ãƒ« ---
class BreakthroughSNN(nn.Module):
    """
    ãƒªã‚«ãƒ¬ãƒ³ãƒˆäºˆæ¸¬ç¬¦å·åŒ–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å®Ÿè£…ã—ãŸéšå±¤çš„SNN
    """
    def __init__(self, vocab_size: int, d_model: int, d_state: int, num_layers: int, time_steps: int, n_head: int, neuron_config: Optional[Dict[str, Any]] = None):
        super().__init__()
        self.time_steps = time_steps
        self.num_layers = num_layers
        self.d_model = d_model
        self.d_state = d_state

        self.token_embedding = nn.Embedding(vocab_size, d_model)
        
        if neuron_config is None:
            neuron_config = {"type": "lif"}
        
        neuron_type = neuron_config.get("type", "lif")
        
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        neuron_class: Type[nn.Module]
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

        if neuron_type == "dendritic":
            neuron_class = DendriticNeuron
            neuron_params = {
                "num_branches": neuron_config.get("num_branches", 4),
                "branch_features": neuron_config.get("branch_features", d_model // 4)
            }
            self.input_encoder = nn.Sequential(
                nn.Linear(d_model, d_model),
                DendriticNeuron(input_features=d_model, **neuron_params)
            )
            print("ğŸ’¡ BreakthroughSNN initialized with Dendritic Neurons.")
        else:  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ "lif"
            neuron_class = AdaptiveLIFNeuron
            neuron_params = {}  # LIFã«è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä¸è¦
            self.input_encoder = nn.Sequential(
                nn.Linear(d_model, d_model),
                AdaptiveLIFNeuron(features=d_model)
            )
        
        self.pc_layers = nn.ModuleList(
            [PredictiveCodingLayer(d_model, d_state, n_head, neuron_class, neuron_params) for _ in range(num_layers)]
        )
        
        self.output_projection = nn.Linear(d_model, vocab_size)

    def forward(
        self,
        input_ids: torch.Tensor,
        return_spikes: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:

        batch_size, seq_len = input_ids.shape
        token_emb = self.token_embedding(input_ids)
        
        states = [torch.zeros(batch_size, self.d_state, device=input_ids.device) for _ in range(self.num_layers)]
        
        total_spikes = torch.tensor(0.0, device=input_ids.device)
        total_mem_potential = torch.tensor(0.0, device=input_ids.device)
        all_logits = []

        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦é †æ–¹å‘ã«å‡¦ç†
        for i in range(seq_len):
            embedded_token = token_emb[:, i, :]
            
            bottom_up_input, _ = self.input_encoder(embedded_token)
            
            layer_errors = []
            layer_mems = []
            
            # ãƒœãƒˆãƒ ã‚¢ãƒƒãƒ—å‡¦ç† (æ¨è«–)
            for j in range(self.num_layers):
                states[j], error, _, mem = self.pc_layers[j](bottom_up_input, states[j])
                bottom_up_input = F.relu(states[j]) # æ¬¡ã®å±¤ã¸ã®å…¥åŠ›ã¯ç¾åœ¨ã®å±¤ã®çŠ¶æ…‹ã‚¹ãƒ‘ã‚¤ã‚¯
                layer_errors.append(error)
                layer_mems.append(mem)
            
            # ãƒˆãƒƒãƒ—ãƒ€ã‚¦ãƒ³å‡¦ç† (ç”Ÿæˆ)
            top_down_signal = states[-1]
            for j in reversed(range(self.num_layers)):
                 _, _, prediction, _ = self.pc_layers[j](torch.zeros_like(bottom_up_input, device=input_ids.device), top_down_signal)
                 top_down_signal = prediction
            
            # æœ€çµ‚çš„ãªå‡ºåŠ›å±¤ã®äºˆæ¸¬ã‹ã‚‰ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—
            logits = self.output_projection(top_down_signal)
            all_logits.append(logits)

            if return_spikes:
                total_spikes += sum(err.mean() for err in layer_errors)
                total_mem_potential += sum(m.abs().mean() for m in layer_mems)

        # (batch_size, seq_len, vocab_size) ã®å½¢çŠ¶ã«ã‚¹ã‚¿ãƒƒã‚¯
        final_logits = torch.stack(all_logits, dim=1)
        
        avg_spikes = total_spikes / seq_len if return_spikes else torch.tensor(0.0)
        avg_mem = total_mem_potential / seq_len if return_spikes else torch.tensor(0.0)

        return final_logits, avg_spikes, avg_mem
# â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

